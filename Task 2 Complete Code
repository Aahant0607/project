# ==================================================================================
# 1. SETUP: INSTALL LIBRARIES
# ==================================================================================
#!pip install -q segmentation-models-pytorch albumentations tacoreader rasterio

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
import segmentation_models_pytorch as smp
import albumentations as A
from albumentations.pytorch import ToTensorV2

import tacoreader
import rasterio as rio
from rasterio.errors import RasterioIOError
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tqdm.notebook import tqdm
import os
import time
from pathlib import Path
import random

# ==================================================================================
# 2. CONFIGURATION (Optimized for Speed)
# ==================================================================================
class Config:
    DATASET_NAME = "tacofoundation:cloudsen12-l1c"
    IMG_SIZE = 256
    BACKBONE = "efficientnet-b2"
    BATCH_SIZE = 32
    NUM_WORKERS = 2
    BANDS = [4, 3, 2]
    VAL_SPLIT = 0.2
    MODEL_ARCH = "Unet"
    MAX_EPOCHS = 50
    LEARNING_RATE = 1e-4
    DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    WEIGHT_DECAY = 1e-2
    EARLY_STOPPING_PATIENCE = 5
    
# ==================================================================================
# 3. STEP 1: DOWNLOAD AND LOCALIZE DATASET (WITH RETRY LOGIC)
# ==================================================================================
def localize_dataset(cfg, num_samples=3000):
    print("--- Step 1: Localizing dataset ---")
    
    LOCAL_IMG_DIR = Path("./temp_data/images")
    LOCAL_MASK_DIR = Path("./temp_data/masks")
    LOCAL_IMG_DIR.mkdir(parents=True, exist_ok=True)
    LOCAL_MASK_DIR.mkdir(parents=True, exist_ok=True)

    # ‚≠ê FIX: Added a retry loop to handle network errors during initial load
    max_retries = 3
    for attempt in range(max_retries):
        try:
            print(f"Loading remote dataset reference (Attempt {attempt + 1}/{max_retries})...")
            tacoreader_dataset = tacoreader.load(cfg.DATASET_NAME)
            print("‚úÖ Remote reference loaded.")
            break 
        except Exception as e:
            if attempt < max_retries - 1:
                print(f"Failed with error: {e}. Retrying in 5 seconds...")
                time.sleep(5)
            else:
                print("Fatal: Could not load dataset after multiple retries.")
                raise e

    all_indices = list(range(len(tacoreader_dataset)))
    demo_indices = all_indices[:num_samples]

    print(f"Downloading {len(demo_indices)} samples to local storage...")
    for idx in tqdm(demo_indices, desc="Downloading Samples"):
        try:
            s2_l1c_path = tacoreader_dataset.read(idx).read(0)
            s2_label_path = tacoreader_dataset.read(idx).read(1)

            with rio.open(s2_l1c_path) as src:
                image = src.read(cfg.BANDS)
            with rio.open(s2_label_path) as src:
                mask = src.read(1)
            
            np.save(LOCAL_IMG_DIR / f"{idx}.npy", image)
            np.save(LOCAL_MASK_DIR / f"{idx}.npy", mask)
        except Exception as e:
            print(f"Warning: Could not download sample {idx}. Skipping. Error: {e}")
            
    print("‚úÖ Dataset localization complete.")
    return demo_indices, LOCAL_IMG_DIR, LOCAL_MASK_DIR

# ==================================================================================
# 4. NEW DATASET CLASS FOR LOCAL FILES
# ==================================================================================
class LocalCloudDataset(Dataset):
    def __init__(self, sample_indices, img_dir, mask_dir, transforms):
        self.sample_indices = [idx for idx in sample_indices if (img_dir / f"{idx}.npy").exists()]
        self.img_dir = img_dir
        self.mask_dir = mask_dir
        self.transforms = transforms

    def __len__(self):
        return len(self.sample_indices)

    def __getitem__(self, idx):
        sample_idx = self.sample_indices[idx]
        
        image_uint16 = np.load(self.img_dir / f"{sample_idx}.npy").transpose(1, 2, 0)
        mask = np.load(self.mask_dir / f"{sample_idx}.npy")

        max_val = 4000
        image_scaled = np.clip(image_uint16, 0, max_val) / max_val
        image = (image_scaled * 255).astype(np.uint8)
        
        binary_mask = np.where((mask == 1) | (mask == 2), 1.0, 0.0).astype(np.float32)
        
        augmented = self.transforms(image=image, mask=binary_mask)
        return augmented['image'], augmented['mask'].unsqueeze(0)

# ==================================================================================
# 5. AUGMENTATIONS, LOSS, AND TRAINING/EVAL LOOPS
# ==================================================================================
def get_transforms(cfg, is_train=True):
    if is_train:
        return A.Compose([
            A.Resize(cfg.IMG_SIZE, cfg.IMG_SIZE),
            A.HorizontalFlip(p=0.5), A.VerticalFlip(p=0.5), A.RandomRotate90(p=0.5),
            A.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, p=0.75),
            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
            ToTensorV2(),
        ])
    else:
        return A.Compose([
            A.Resize(cfg.IMG_SIZE, cfg.IMG_SIZE),
            A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),
            ToTensorV2(),
        ])

class SegmentationLoss(nn.Module):
    def __init__(self):
        super().__init__()
        self.focal = smp.losses.FocalLoss(mode='binary')
        self.dice = smp.losses.DiceLoss(mode='binary')
    def forward(self, y_pred, y_true):
        return 0.25 * self.focal(y_pred, y_true) + 0.75 * self.dice(y_pred, y_true)

def train_one_epoch(model, loader, optimizer, loss_fn, device, scaler):
    model.train()
    total_loss = 0.0
    for images, masks in tqdm(loader, desc="Training", leave=False):
        images, masks = images.to(device), masks.to(device)
        optimizer.zero_grad(set_to_none=True)
        with autocast():
            preds = model(images)
            loss = loss_fn(preds, masks)
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
        total_loss += loss.item()
    return total_loss / len(loader)

def evaluate(model, loader, loss_fn, device):
    model.eval()
    total_loss = 0.0
    all_preds, all_masks = [], []
    with torch.no_grad():
        for images, masks in tqdm(loader, desc="Evaluating", leave=False):
            images, masks = images.to(device), masks.to(device)
            with autocast():
                preds = model(images)
                loss = loss_fn(preds, masks)
            total_loss += loss.item()
            all_preds.append(torch.sigmoid(preds))
            all_masks.append(masks)
    if not all_preds: return 0.0, 0.0
    all_preds, all_masks = torch.cat(all_preds).cpu(), torch.cat(all_masks).cpu()
    tp, fp, fn, tn = smp.metrics.get_stats((all_preds > 0.5).long(), all_masks.long(), mode='binary')
    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro')
    return total_loss / len(loader), iou_score

# ==================================================================================
# 6. MAIN EXECUTION BLOCK
# ==================================================================================
if __name__ == '__main__':
    cfg = Config()
    
    # STEP 1: Run the localization process
    #demo_indices, img_dir, mask_dir = localize_dataset(cfg)
    # STEP 1: Comment out the localization process to prevent re-downloading
    # demo_indices, img_dir, mask_dir = localize_dataset(cfg)
    # 2. Delete the old model checkpoint to prevent errors with new architecture
    model_path = Path('best_model.pth')
    if model_path.exists():
        print(f"üóëÔ∏è Deleting old model checkpoint: {model_path}")
        model_path.unlink()
    else:
        print("üëç No old model checkpoint found. Starting fresh.")
    
    # Manually define the absolute paths for your Kaggle environment
    print("--- Step 1: Bypassing download, using existing Kaggle data ---")
    img_dir = Path("/kaggle/working/temp_data/images")
    mask_dir = Path("/kaggle/working/temp_data/masks")
    
    # Recreate the list of indices by reading the filenames of the downloaded samples
    demo_indices = [int(p.stem) for p in img_dir.glob("*.npy")]
    print(f"‚úÖ Found {len(demo_indices)} local samples at {img_dir}")
    
    # STEP 2: Train on the localized data
    print("\n--- Step 2: Training on localized data ---")
    train_indices, val_indices = train_test_split(demo_indices, test_size=cfg.VAL_SPLIT, random_state=42)

    train_dataset = LocalCloudDataset(train_indices, img_dir, mask_dir, get_transforms(cfg, is_train=True))
    val_dataset = LocalCloudDataset(val_indices, img_dir, mask_dir, get_transforms(cfg, is_train=False))

    train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=cfg.NUM_WORKERS, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=cfg.NUM_WORKERS, pin_memory=True)

    print(f"Initializing {cfg.MODEL_ARCH} with {cfg.BACKBONE} backbone...")
    model = smp.Unet(encoder_name=cfg.BACKBONE, encoder_weights="imagenet", in_channels=3, classes=1).to(cfg.DEVICE)
    loss_fn = SegmentationLoss().to(cfg.DEVICE)
    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.LEARNING_RATE, weight_decay=cfg.WEIGHT_DECAY)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)
    scaler = GradScaler()
    
    best_iou, patience_counter = 0.0, 0
    for epoch in range(cfg.MAX_EPOCHS):
        print(f"--- Epoch {epoch+1}/{cfg.MAX_EPOCHS} | LR: {optimizer.param_groups[0]['lr']:.6f} ---")
        train_loss = train_one_epoch(model, train_loader, optimizer, loss_fn, cfg.DEVICE, scaler)
        val_loss, val_iou = evaluate(model, val_loader, loss_fn, cfg.DEVICE)
        scheduler.step(val_loss)
        
        print(f"  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | Val IoU: {val_iou:.4f}")
        
        if val_iou > best_iou:
            best_iou, patience_counter = val_iou, 0
            torch.save(model.state_dict(), 'best_model.pth')
            print(f"  ‚ú® New best model saved with IoU: {best_iou:.4f}")
        else:
            patience_counter += 1
            print(f"  Patience: {patience_counter}/{cfg.EARLY_STOPPING_PATIENCE}")
        
        if patience_counter >= cfg.EARLY_STOPPING_PATIENCE:
            print(f"\nStopping early after {patience_counter} epochs with no improvement.")
            break
            
    print(f"\n‚úÖ Training finished! Best validation IoU: {best_iou:.4f}")

import torch
import segmentation_models_pytorch as smp
from tqdm.notebook import tqdm
import matplotlib.pyplot as plt
import random
from pathlib import Path
from torch.cuda.amp import autocast

# ==================================================================================
# CORRECTED STEP 7: FINAL EVALUATION & VISUALIZATION
# ==================================================================================
print("\n--- Step 7: Final Evaluation and Visualization ---")

# --- Load the best performing model's weights ---
# This assumes 'model', 'cfg', and 'val_dataset' are still available in your environment
model.load_state_dict(torch.load('best_model.pth', map_location=cfg.DEVICE))

# --- Define the corrected evaluation function ---
def evaluate_all_metrics(model, loader, device):
    model.eval()
    all_preds, all_masks = [], []
    with torch.no_grad():
        for images, masks in tqdm(loader, desc="Final Evaluation", leave=False):
            images, masks = images.to(device), masks.to(device)
            with autocast():
                preds = model(images)
            all_preds.append(torch.sigmoid(preds))
            all_masks.append(masks)

    all_preds = torch.cat(all_preds).cpu()
    all_masks = torch.cat(all_masks).cpu()
    
    # +++ THIS IS THE CRITICAL FIX +++
    # Apply a 0.5 threshold before converting to long type
    tp, fp, fn, tn = smp.metrics.get_stats((all_preds > 0.5).long(), all_masks.long(), mode='binary')
    
    iou_score = smp.metrics.iou_score(tp, fp, fn, tn, reduction='micro')
    f1_score = smp.metrics.f1_score(tp, fp, fn, tn, reduction='micro')
    accuracy = smp.metrics.accuracy(tp, fp, fn, tn, reduction='micro')
    
    return iou_score, f1_score, accuracy

# --- Run the corrected evaluation ---
val_iou, val_f1, val_acc = evaluate_all_metrics(model, val_loader, cfg.DEVICE)
print("\n--- Final Model Performance on Validation Set ---")
print(f"üìä IoU Score: {val_iou:.4f}")
print(f"üìä F1 Score: {val_f1:.4f}")
print(f"üìä Pixel Accuracy: {val_acc:.4f}")
print("-------------------------------------------------")


# --- Visualize predictions for 5 random samples ---
print("\nGenerating visual results for 5 random samples...")
model.eval()
num_samples_to_show = 5
# This needs the val_dataset object from your main script
sample_indices_to_show = random.sample(range(len(val_dataset)), num_samples_to_show)

def denormalize(tensor):
    tensor = tensor.clone().permute(1, 2, 0)
    tensor.mul_(torch.tensor([0.5, 0.5, 0.5])).add_(torch.tensor([0.5, 0.5, 0.5]))
    return torch.clamp(tensor, 0, 1)

for i in sample_indices_to_show:
    image_tensor, true_mask = val_dataset[i]
    
    with torch.no_grad():
        image_tensor_batch = image_tensor.unsqueeze(0).to(cfg.DEVICE)
        pred_mask = model(image_tensor_batch)
        pred_mask = (torch.sigmoid(pred_mask) > 0.5).squeeze(0).cpu()

    image_to_plot = denormalize(image_tensor)
    true_mask_to_plot = true_mask.squeeze()
    pred_mask_to_plot = pred_mask.squeeze()

    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6))
    ax1.imshow(image_to_plot)
    ax1.set_title("Original Image")
    ax1.axis('off')

    ax2.imshow(true_mask_to_plot, cmap='gray')
    ax2.set_title("True Mask (Ground Truth)")
    ax2.axis('off')

    ax3.imshow(pred_mask_to_plot, cmap='gray')
    ax3.set_title("Predicted Mask")
    ax3.axis('off')

    plt.tight_layout()
    plt.show()

print("\n‚úÖ Visualization finished.")
